---
title: "Automated AWS Log Analysis with Serverless: Building Intelligent Monitoring at Scale"
date: 2025-06-14T10:00:00-07:00
draft: true
categories: ["Cloud Computing", "DevOps and Monitoring"]
tags:
- AWS
- TypeScript
- Serverless
- Monitoring
- CloudWatch
- CloudTrail
- Security
- Automation
series: "AWS and Typescript"
---

The explosion of cloud-native applications has created a monitoring challenge: how do you effectively analyze the massive volumes of log data generated by distributed systems? Traditional approaches fall short when dealing with terabytes of CloudTrail events, CloudWatch logs, and application metrics. This guide demonstrates how to build an intelligent, automated log analysis system using AWS serverless technologies that can process, analyze, and report on your infrastructure activity with minimal operational overhead.

## The Challenge: Log Analysis at Scale

Modern AWS environments generate enormous amounts of log data. A typical enterprise application might produce over 10,000 CloudTrail API calls per hour, gigabytes of application logs daily through CloudWatch, millions of network events via VPC Flow logs, and thousands of custom application metrics. This volume of data creates a significant challenge for security and operations teams who need to maintain visibility into their infrastructure.

Manual analysis of this data volume is simply impossible at scale, and traditional monitoring tools often miss subtle patterns that could indicate security threats or operational issues. These tools frequently require extensive configuration overhead and struggle to adapt to the dynamic nature of cloud environments. What modern organizations need is an intelligent system that can automatically identify anomalies, detect security threats, and provide actionable insights without requiring constant human intervention.

## Solution Architecture

Our serverless log analysis system leverages multiple AWS services to create a scalable, cost-effective monitoring solution:

{{< plantuml id="log-analysis-architecture" >}}
@startuml
!theme aws-orange
title Automated Log Analysis Architecture

package "Data Sources" {
  [CloudTrail] as ct
  [CloudWatch Logs] as cwl
  [VPC Flow Logs] as vpc
  [Application Logs] as app
}

package "Ingestion Layer" {
  [Kinesis Data Streams] as kds
  [Kinesis Data Firehose] as kdf
}

package "Storage Layer" {
  [S3 Raw Data] as s3raw
  [S3 Processed] as s3proc
  [DynamoDB Analysis] as ddb
}

package "Processing Layer" {
  [Lambda Processor] as proc
  [Lambda Analyzer] as analyzer
  [Step Functions] as sf
}

package "Intelligence Layer" {
  [Amazon Bedrock] as bedrock
  [OpenSearch] as os
  [Lambda AI Analysis] as ai
}

package "Notification Layer" {
  [SNS] as sns
  [SES] as ses
  [Slack Integration] as slack
}

ct --> kds
cwl --> kds
vpc --> kds
app --> kds

kds --> kdf
kdf --> s3raw

s3raw --> proc : S3 Event
proc --> analyzer
proc --> s3proc : Processed Data
analyzer --> sf
sf --> ai
ai --> bedrock

analyzer --> ddb
analyzer --> os

ai --> sns
sns --> ses
sns --> slack
@enduml
{{< /plantuml >}}

## Implementation: Core Components

Let's build each component of our automated log analysis system, starting with the data processing pipeline.

### CloudTrail Event Processor

Our first Lambda function serves as the foundation of the analysis pipeline, processing CloudTrail events to extract meaningful patterns and detect security anomalies. This processor examines each event for suspicious characteristics, calculates risk scores, and stores structured data for further analysis.

```typescript
import { S3Event, S3Handler } from 'aws-lambda';
import { S3Client, GetObjectCommand } from '@aws-sdk/client-s3';
import { DynamoDBClient, PutItemCommand } from '@aws-sdk/client-dynamodb';
import { gzipSync, gunzipSync } from 'zlib';

interface CloudTrailRecord {
  eventTime: string;
  eventName: string;
  sourceIPAddress: string;
  userIdentity: {
    type: string;
    principalId?: string;
    arn?: string;
    userName?: string;
  };
  eventSource: string;
  errorCode?: string;
  errorMessage?: string;
  responseElements?: any;
}

interface ProcessedEvent {
  timestamp: string;
  eventName: string;
  sourceIP: string;
  userType: string;
  userId: string;
  service: string;
  isError: boolean;
  riskScore: number;
  anomalyFlags: string[];
}

export class CloudTrailProcessor {
  private s3Client: S3Client;
  private dynamoClient: DynamoDBClient;
  
  constructor() {
    this.s3Client = new S3Client({ region: process.env.AWS_REGION });
    this.dynamoClient = new DynamoDBClient({ region: process.env.AWS_REGION });
  }

  async processS3Event(event: S3Event): Promise<void> {
    for (const record of event.Records) {
      const bucket = record.s3.bucket.name;
      const key = record.s3.object.key;
      
      try {
        const cloudTrailData = await this.getCloudTrailData(bucket, key);
        const processedEvents = await this.analyzeEvents(cloudTrailData);
        await this.storeAnalysis(processedEvents);
      } catch (error) {
        console.error(`Error processing ${key}:`, error);
        throw error;
      }
    }
  }

  private async getCloudTrailData(bucket: string, key: string): Promise<CloudTrailRecord[]> {
    const command = new GetObjectCommand({ Bucket: bucket, Key: key });
    const response = await this.s3Client.send(command);
    
    if (!response.Body) {
      throw new Error('No data received from S3');
    }
    
    const bodyBytes = await response.Body.transformToByteArray();
    const decompressed = gunzipSync(Buffer.from(bodyBytes));
    const data = JSON.parse(decompressed.toString());
    
    return data.Records;
  }

  private async analyzeEvents(records: CloudTrailRecord[]): Promise<ProcessedEvent[]> {
    const processed: ProcessedEvent[] = [];
    
    for (const record of records) {
      const event = await this.processEvent(record);
      processed.push(event);
    }
    
    return processed;
  }

  private async processEvent(record: CloudTrailRecord): Promise<ProcessedEvent> {
    const anomalyFlags = await this.detectAnomalies(record);
    const riskScore = this.calculateRiskScore(record, anomalyFlags);
    
    return {
      timestamp: record.eventTime,
      eventName: record.eventName,
      sourceIP: record.sourceIPAddress,
      userType: record.userIdentity.type,
      userId: record.userIdentity.principalId || record.userIdentity.userName || 'unknown',
      service: record.eventSource,
      isError: !!(record.errorCode || record.errorMessage),
      riskScore,
      anomalyFlags
    };
  }

  private async detectAnomalies(record: CloudTrailRecord): Promise<string[]> {
    const flags: string[] = [];
    
    // Unusual time detection
    const eventHour = new Date(record.eventTime).getHours();
    if (eventHour < 6 || eventHour > 22) {
      flags.push('UNUSUAL_TIME');
    }
    
    // Geographic anomaly (simplified)
    if (this.isUnusualLocation(record.sourceIPAddress)) {
      flags.push('UNUSUAL_LOCATION');
    }
    
    // High-risk actions
    const highRiskActions = [
      'DeleteUser', 'DeleteRole', 'PutUserPolicy', 'CreateUser',
      'AttachUserPolicy', 'ModifyDBInstance', 'TerminateInstances'
    ];
    
    if (highRiskActions.includes(record.eventName)) {
      flags.push('HIGH_RISK_ACTION');
    }
    
    // Root user activity
    if (record.userIdentity.type === 'Root') {
      flags.push('ROOT_USER_ACTIVITY');
    }
    
    // Failed authentication attempts
    if (record.errorCode === 'SigninFailure' || record.errorCode === 'InvalidUserID.NotFound') {
      flags.push('FAILED_AUTHENTICATION');
    }
    
    return flags;
  }

  private calculateRiskScore(record: CloudTrailRecord, anomalyFlags: string[]): number {
    let score = 0;
    
    // Base score for errors
    if (record.errorCode || record.errorMessage) {
      score += 3;
    }
    
    // Anomaly flag scoring
    const flagScores: Record<string, number> = {
      'UNUSUAL_TIME': 2,
      'UNUSUAL_LOCATION': 4,
      'HIGH_RISK_ACTION': 5,
      'ROOT_USER_ACTIVITY': 6,
      'FAILED_AUTHENTICATION': 3
    };
    
    for (const flag of anomalyFlags) {
      score += flagScores[flag] || 1;
    }
    
    return Math.min(score, 10); // Cap at 10
  }

  private isUnusualLocation(ip: string): boolean {
    // In a real implementation, you'd check against:
    // - Known corporate IP ranges
    // - Geographic IP databases
    // - Historical access patterns
    
    // Simplified example: flag non-private IPs as potentially unusual
    const privateRanges = [
      /^10\./,
      /^172\.(1[6-9]|2[0-9]|3[01])\./,
      /^192\.168\./,
      /^127\./
    ];
    
    return !privateRanges.some(range => range.test(ip));
  }

  private async storeAnalysis(events: ProcessedEvent[]): Promise<void> {
    const tableName = process.env.ANALYSIS_TABLE_NAME;
    if (!tableName) {
      throw new Error('ANALYSIS_TABLE_NAME environment variable is required');
    }

    for (const event of events) {
      const command = new PutItemCommand({
        TableName: tableName,
        Item: {
          id: { S: `${event.timestamp}-${event.eventName}-${Math.random()}` },
          timestamp: { S: event.timestamp },
          eventName: { S: event.eventName },
          sourceIP: { S: event.sourceIP },
          userType: { S: event.userType },
          userId: { S: event.userId },
          service: { S: event.service },
          isError: { BOOL: event.isError },
          riskScore: { N: event.riskScore.toString() },
          anomalyFlags: { SS: event.anomalyFlags }
        }
      });
      
      await this.dynamoClient.send(command);
    }
  }
}

export const handler: S3Handler = async (event: S3Event) => {
  const processor = new CloudTrailProcessor();
  await processor.processS3Event(event);
};
```

### AI-Powered Analysis Engine

The intelligence layer of our system leverages Amazon Bedrock to transform raw security data into actionable insights. This engine aggregates patterns from the processed events, identifies anomalies, and generates human-readable reports with specific recommendations for security improvements.

```typescript
import { DynamoDBClient, ScanCommand } from '@aws-sdk/client-dynamodb';
import { BedrockRuntimeClient, InvokeModelCommand } from '@aws-sdk/client-bedrock-runtime';
import { SNSClient, PublishCommand } from '@aws-sdk/client-sns';

interface AnalysisResult {
  timeframe: string;
  totalEvents: number;
  highRiskEvents: number;
  anomalies: AnomalyReport[];
  insights: string[];
  recommendations: string[];
  summary: string;
}

interface AnomalyReport {
  type: string;
  count: number;
  severity: 'LOW' | 'MEDIUM' | 'HIGH' | 'CRITICAL';
  details: string;
}

export class AIAnalysisEngine {
  private dynamoClient: DynamoDBClient;
  private bedrockClient: BedrockRuntimeClient;
  private snsClient: SNSClient;

  constructor() {
    this.dynamoClient = new DynamoDBClient({ region: process.env.AWS_REGION });
    this.bedrockClient = new BedrockRuntimeClient({ region: process.env.AWS_REGION });
    this.snsClient = new SNSClient({ region: process.env.AWS_REGION });
  }

  async generateDailyReport(): Promise<AnalysisResult> {
    // Get events from the last 24 hours
    const events = await this.getRecentEvents();
    
    // Analyze patterns and anomalies
    const analysis = await this.analyzePatterns(events);
    
    // Generate AI insights
    const aiInsights = await this.generateAIInsights(analysis);
    
    // Create comprehensive report
    const report = await this.createReport(analysis, aiInsights);
    
    // Send notifications for high-risk findings
    await this.sendNotifications(report);
    
    return report;
  }

  private async getRecentEvents(): Promise<any[]> {
    const tableName = process.env.ANALYSIS_TABLE_NAME;
    const twentyFourHoursAgo = new Date(Date.now() - 24 * 60 * 60 * 1000).toISOString();
    
    const command = new ScanCommand({
      TableName: tableName,
      FilterExpression: '#timestamp > :timeframe',
      ExpressionAttributeNames: {
        '#timestamp': 'timestamp'
      },
      ExpressionAttributeValues: {
        ':timeframe': { S: twentyFourHoursAgo }
      }
    });
    
    const response = await this.dynamoClient.send(command);
    return response.Items || [];
  }

  private async analyzePatterns(events: any[]): Promise<any> {
    const analysis = {
      totalEvents: events.length,
      highRiskEvents: events.filter(e => parseFloat(e.riskScore.N) >= 7).length,
      errorEvents: events.filter(e => e.isError.BOOL).length,
      anomalies: this.categorizeAnomalies(events),
      timeDistribution: this.analyzeTimeDistribution(events),
      userActivity: this.analyzeUserActivity(events),
      serviceActivity: this.analyzeServiceActivity(events)
    };
    
    return analysis;
  }

  private categorizeAnomalies(events: any[]): AnomalyReport[] {
    const anomalyGroups: Record<string, number> = {};
    
    for (const event of events) {
      if (event.anomalyFlags?.SS) {
        for (const flag of event.anomalyFlags.SS) {
          anomalyGroups[flag] = (anomalyGroups[flag] || 0) + 1;
        }
      }
    }
    
    return Object.entries(anomalyGroups).map(([type, count]) => ({
      type,
      count,
      severity: this.determineSeverity(type, count),
      details: this.getAnomalyDescription(type, count)
    }));
  }

  private determineSeverity(type: string, count: number): 'LOW' | 'MEDIUM' | 'HIGH' | 'CRITICAL' {
    const severityMap: Record<string, number> = {
      'ROOT_USER_ACTIVITY': 8,
      'HIGH_RISK_ACTION': 7,
      'FAILED_AUTHENTICATION': 6,
      'UNUSUAL_LOCATION': 5,
      'UNUSUAL_TIME': 3
    };
    
    const baseSeverity = severityMap[type] || 2;
    const finalScore = baseSeverity + Math.log10(count);
    
    if (finalScore >= 8) return 'CRITICAL';
    if (finalScore >= 6) return 'HIGH';
    if (finalScore >= 4) return 'MEDIUM';
    return 'LOW';
  }

  private getAnomalyDescription(type: string, count: number): string {
    const descriptions: Record<string, string> = {
      'ROOT_USER_ACTIVITY': `Root user accessed AWS ${count} times in the last 24 hours`,
      'HIGH_RISK_ACTION': `${count} high-risk administrative actions detected`,
      'FAILED_AUTHENTICATION': `${count} failed authentication attempts observed`,
      'UNUSUAL_LOCATION': `Access from ${count} unusual geographic locations`,
      'UNUSUAL_TIME': `${count} activities during unusual hours (outside 6 AM - 10 PM)`
    };
    
    return descriptions[type] || `${count} instances of ${type} detected`;
  }

  private analyzeTimeDistribution(events: any[]): Record<number, number> {
    const hourlyDistribution: Record<number, number> = {};
    
    for (const event of events) {
      const hour = new Date(event.timestamp.S).getHours();
      hourlyDistribution[hour] = (hourlyDistribution[hour] || 0) + 1;
    }
    
    return hourlyDistribution;
  }

  private analyzeUserActivity(events: any[]): Record<string, number> {
    const userActivity: Record<string, number> = {};
    
    for (const event of events) {
      const userId = event.userId.S;
      userActivity[userId] = (userActivity[userId] || 0) + 1;
    }
    
    return userActivity;
  }

  private analyzeServiceActivity(events: any[]): Record<string, number> {
    const serviceActivity: Record<string, number> = {};
    
    for (const event of events) {
      const service = event.service.S;
      serviceActivity[service] = (serviceActivity[service] || 0) + 1;
    }
    
    return serviceActivity;
  }

  private async generateAIInsights(analysis: any): Promise<{ insights: string[], recommendations: string[], summary: string }> {
    const prompt = this.buildAnalysisPrompt(analysis);
    
    const command = new InvokeModelCommand({
      modelId: 'anthropic.claude-3-sonnet-20240229-v1:0',
      contentType: 'application/json',
      accept: 'application/json',
      body: JSON.stringify({
        anthropic_version: 'bedrock-2023-05-31',
        max_tokens: 4000,
        messages: [{
          role: 'user',
          content: prompt
        }]
      })
    });
    
    const response = await this.bedrockClient.send(command);
    const responseData = JSON.parse(new TextDecoder().decode(response.body));
    
    return this.parseAIResponse(responseData.content[0].text);
  }

  private buildAnalysisPrompt(analysis: any): string {
    return `
As a cybersecurity analyst, analyze the following AWS CloudTrail data from the last 24 hours and provide:

1. Key insights about security posture
2. Specific recommendations for improvement
3. A concise executive summary

Data Summary:
- Total Events: ${analysis.totalEvents}
- High Risk Events: ${analysis.highRiskEvents}
- Error Events: ${analysis.errorEvents}
- Anomalies Detected: ${JSON.stringify(analysis.anomalies, null, 2)}
- Hourly Distribution: ${JSON.stringify(analysis.timeDistribution, null, 2)}
- Top User Activity: ${JSON.stringify(Object.entries(analysis.userActivity).slice(0, 10), null, 2)}
- Service Activity: ${JSON.stringify(analysis.serviceActivity, null, 2)}

Please format your response as JSON with the following structure:
{
  "insights": ["insight 1", "insight 2", ...],
  "recommendations": ["recommendation 1", "recommendation 2", ...],
  "summary": "executive summary paragraph"
}

Focus on actionable insights and prioritize security concerns.`;
  }

  private parseAIResponse(responseText: string): { insights: string[], recommendations: string[], summary: string } {
    try {
      return JSON.parse(responseText);
    } catch (error) {
      // Fallback if AI doesn't return valid JSON
      return {
        insights: ['AI analysis parsing failed - manual review required'],
        recommendations: ['Review AI response format and adjust prompt'],
        summary: responseText.substring(0, 500) + '...'
      };
    }
  }

  private async createReport(analysis: any, aiInsights: any): Promise<AnalysisResult> {
    return {
      timeframe: 'Last 24 hours',
      totalEvents: analysis.totalEvents,
      highRiskEvents: analysis.highRiskEvents,
      anomalies: analysis.anomalies,
      insights: aiInsights.insights,
      recommendations: aiInsights.recommendations,
      summary: aiInsights.summary
    };
  }

  private async sendNotifications(report: AnalysisResult): Promise<void> {
    const criticalAnomalies = report.anomalies.filter(a => a.severity === 'CRITICAL');
    const highRiskAnomalies = report.anomalies.filter(a => a.severity === 'HIGH');
    
    if (criticalAnomalies.length > 0 || highRiskAnomalies.length > 0) {
      const message = this.formatAlertMessage(report, criticalAnomalies, highRiskAnomalies);
      
      const command = new PublishCommand({
        TopicArn: process.env.ALERT_TOPIC_ARN,
        Subject: 'AWS Security Alert - High Risk Activity Detected',
        Message: message
      });
      
      await this.snsClient.send(command);
    }
  }

  private formatAlertMessage(report: AnalysisResult, critical: AnomalyReport[], high: AnomalyReport[]): string {
    let message = `🚨 AWS Security Alert - ${report.timeframe}\n\n`;
    
    message += `📊 Summary:\n`;
    message += `• Total Events: ${report.totalEvents}\n`;
    message += `• High Risk Events: ${report.highRiskEvents}\n\n`;
    
    if (critical.length > 0) {
      message += `🔴 CRITICAL Anomalies:\n`;
      critical.forEach(anomaly => {
        message += `• ${anomaly.details}\n`;
      });
      message += '\n';
    }
    
    if (high.length > 0) {
      message += `🟠 HIGH Risk Anomalies:\n`;
      high.forEach(anomaly => {
        message += `• ${anomaly.details}\n`;
      });
      message += '\n';
    }
    
    message += `💡 Key Insights:\n`;
    report.insights.slice(0, 3).forEach(insight => {
      message += `• ${insight}\n`;
    });
    
    message += `\n📋 Recommendations:\n`;
    report.recommendations.slice(0, 3).forEach(rec => {
      message += `• ${rec}\n`;
    });
    
    return message;
  }
}

export const handler = async () => {
  const engine = new AIAnalysisEngine();
  return await engine.generateDailyReport();
};
```

### Workflow Orchestration with Step Functions

Coordinating the various components of our analysis system requires a robust orchestration layer. Step Functions provide the workflow management needed to ensure proper sequencing of data processing, analysis, and notification tasks while handling error conditions gracefully.

```typescript
import { SFNClient, StartExecutionCommand } from '@aws-sdk/client-sfn';

interface AnalysisWorkflowInput {
  timeframe: string;
  analysisType: 'daily' | 'weekly' | 'monthly' | 'custom';
  includeAI: boolean;
  notificationLevel: 'all' | 'high_risk_only' | 'critical_only';
}

export class AnalysisOrchestrator {
  private sfnClient: SFNClient;

  constructor() {
    this.sfnClient = new SFNClient({ region: process.env.AWS_REGION });
  }

  async startAnalysis(input: AnalysisWorkflowInput): Promise<string> {
    const stateMachineArn = process.env.ANALYSIS_STATE_MACHINE_ARN;
    if (!stateMachineArn) {
      throw new Error('ANALYSIS_STATE_MACHINE_ARN environment variable is required');
    }

    const command = new StartExecutionCommand({
      stateMachineArn,
      input: JSON.stringify(input),
      name: `analysis-${Date.now()}`
    });

    const response = await this.sfnClient.send(command);
    return response.executionArn!;
  }
}

// Handler for scheduled daily analysis
export const scheduledAnalysisHandler = async () => {
  const orchestrator = new AnalysisOrchestrator();
  
  const input: AnalysisWorkflowInput = {
    timeframe: 'daily',
    analysisType: 'daily',
    includeAI: true,
    notificationLevel: 'high_risk_only'
  };
  
  return await orchestrator.startAnalysis(input);
};
```

### Infrastructure as Code Implementation

Defining your infrastructure using AWS CDK ensures reproducible deployments and maintainable infrastructure. The CDK stack encompasses all necessary AWS resources, from Lambda functions and DynamoDB tables to IAM policies and event triggers, creating a complete serverless monitoring solution.

```typescript
import * as cdk from 'aws-cdk-lib';
import * as lambda from 'aws-cdk-lib/aws-lambda';
import * as dynamodb from 'aws-cdk-lib/aws-dynamodb';
import * as s3 from 'aws-cdk-lib/aws-s3';
import * as s3n from 'aws-cdk-lib/aws-s3-notifications';
import * as sns from 'aws-cdk-lib/aws-sns';
import * as events from 'aws-cdk-lib/aws-events';
import * as targets from 'aws-cdk-lib/aws-events-targets';
import * as stepfunctions from 'aws-cdk-lib/aws-stepfunctions';
import * as sfnTasks from 'aws-cdk-lib/aws-stepfunctions-tasks';
import * as iam from 'aws-cdk-lib/aws-iam';
import { Construct } from 'constructs';

export class LogAnalysisStack extends cdk.Stack {
  constructor(scope: Construct, id: string, props?: cdk.StackProps) {
    super(scope, id, props);

    // DynamoDB table for storing analysis results
    const analysisTable = new dynamodb.Table(this, 'AnalysisTable', {
      tableName: 'log-analysis-results',
      partitionKey: { name: 'id', type: dynamodb.AttributeType.STRING },
      sortKey: { name: 'timestamp', type: dynamodb.AttributeType.STRING },
      removalPolicy: cdk.RemovalPolicy.DESTROY,
      billingMode: dynamodb.BillingMode.PAY_PER_REQUEST,
      timeToLiveAttribute: 'ttl'
    });

    // S3 bucket for CloudTrail logs
    const logsBucket = new s3.Bucket(this, 'CloudTrailLogsBucket', {
      bucketName: `cloudtrail-logs-${this.account}-${this.region}`,
      removalPolicy: cdk.RemovalPolicy.DESTROY,
      autoDeleteObjects: true
    });

    // SNS topic for alerts
    const alertTopic = new sns.Topic(this, 'SecurityAlertTopic', {
      topicName: 'security-alerts',
      displayName: 'Security Alert Notifications'
    });

    // Lambda function for processing CloudTrail events
    const processorFunction = new lambda.Function(this, 'CloudTrailProcessor', {
      runtime: lambda.Runtime.NODEJS_18_X,
      handler: 'cloudtrail-processor.handler',
      code: lambda.Code.fromAsset('dist/lambda'),
      timeout: cdk.Duration.minutes(5),
      memorySize: 512,
      environment: {
        ANALYSIS_TABLE_NAME: analysisTable.tableName,
        ALERT_TOPIC_ARN: alertTopic.topicArn
      }
    });

    // Lambda function for AI analysis
    const aiAnalysisFunction = new lambda.Function(this, 'AIAnalysisEngine', {
      runtime: lambda.Runtime.NODEJS_18_X,
      handler: 'ai-analysis.handler',
      code: lambda.Code.fromAsset('dist/lambda'),
      timeout: cdk.Duration.minutes(15),
      memorySize: 1024,
      environment: {
        ANALYSIS_TABLE_NAME: analysisTable.tableName,
        ALERT_TOPIC_ARN: alertTopic.topicArn
      }
    });

    // Lambda function for orchestration
    const orchestratorFunction = new lambda.Function(this, 'AnalysisOrchestrator', {
      runtime: lambda.Runtime.NODEJS_18_X,
      handler: 'orchestrator.scheduledAnalysisHandler',
      code: lambda.Code.fromAsset('dist/lambda'),
      timeout: cdk.Duration.minutes(2),
      memorySize: 256
    });

    // Grant permissions
    analysisTable.grantReadWriteData(processorFunction);
    analysisTable.grantReadData(aiAnalysisFunction);
    logsBucket.grantRead(processorFunction);
    alertTopic.grantPublish(processorFunction);
    alertTopic.grantPublish(aiAnalysisFunction);

    // Bedrock permissions for AI analysis
    aiAnalysisFunction.addToRolePolicy(
      new iam.PolicyStatement({
        effect: iam.Effect.ALLOW,
        actions: ['bedrock:InvokeModel'],
        resources: [`arn:aws:bedrock:${this.region}::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0`]
      })
    );

    // S3 event notification to trigger processing
    logsBucket.addEventNotification(
      s3.EventType.OBJECT_CREATED,
      new s3n.LambdaDestination(processorFunction),
      { prefix: 'AWSLogs/', suffix: '.json.gz' }
    );

    // Step Functions state machine for orchestration
    const processTask = new sfnTasks.LambdaInvoke(this, 'ProcessCloudTrailLogs', {
      lambdaFunction: processorFunction,
      outputPath: '$.Payload'
    });

    const aiAnalysisTask = new sfnTasks.LambdaInvoke(this, 'RunAIAnalysis', {
      lambdaFunction: aiAnalysisFunction,
      outputPath: '$.Payload'
    });

    const definition = processTask.next(aiAnalysisTask);

    const stateMachine = new stepfunctions.StateMachine(this, 'AnalysisStateMachine', {
      definition,
      timeout: cdk.Duration.minutes(30)
    });

    // Grant Step Functions permissions to orchestrator
    stateMachine.grantStartExecution(orchestratorFunction);
    orchestratorFunction.addEnvironment('ANALYSIS_STATE_MACHINE_ARN', stateMachine.stateMachineArn);

    // CloudWatch Event Rule for daily execution
    const dailyRule = new events.Rule(this, 'DailyAnalysisRule', {
      schedule: events.Schedule.cron({ hour: '6', minute: '0' }) // 6 AM daily
    });

    dailyRule.addTarget(new targets.LambdaFunction(orchestratorFunction));

    // Outputs
    new cdk.CfnOutput(this, 'AnalysisTableName', {
      value: analysisTable.tableName,
      description: 'DynamoDB table for analysis results'
    });

    new cdk.CfnOutput(this, 'AlertTopicArn', {
      value: alertTopic.topicArn,
      description: 'SNS topic for security alerts'
    });

    new cdk.CfnOutput(this, 'LogsBucketName', {
      value: logsBucket.bucketName,
      description: 'S3 bucket for CloudTrail logs'
    });
  }
}
```

## Deployment and Configuration

### Project Setup

To begin implementing this log analysis system, you'll need to initialize a TypeScript project with the appropriate AWS SDK dependencies. The project structure requires both development and runtime dependencies to support TypeScript compilation and AWS service integration.

```bash
# Initialize project
npm init -y
npm install -D typescript @types/node @types/aws-lambda
npm install @aws-sdk/client-s3 @aws-sdk/client-dynamodb @aws-sdk/client-bedrock-runtime @aws-sdk/client-sns @aws-sdk/client-sfn

# CDK dependencies
npm install -D aws-cdk-lib constructs
```

### TypeScript Configuration

Your TypeScript configuration needs to target modern JavaScript features while maintaining compatibility with the Node.js Lambda runtime. The configuration should enable strict type checking to catch potential errors early in the development process.

```json
{
  "compilerOptions": {
    "target": "ES2020",
    "module": "commonjs",
    "lib": ["ES2020"],
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,
    "outDir": "./dist",
    "rootDir": "./src"
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "dist"]
}
```

### Build Configuration

Streamlining the build and deployment process requires setting up appropriate scripts in your package.json file. These scripts will handle TypeScript compilation and CDK deployment in a single command chain.

```json
{
  "scripts": {
    "build": "tsc",
    "deploy": "npm run build && cdk deploy",
    "watch": "tsc -w"
  }
}
```

### CloudTrail Configuration

Configuring CloudTrail to deliver logs to your designated S3 bucket establishes the foundation for your automated analysis pipeline. The trail should capture global service events and operate across all regions to ensure comprehensive coverage of your AWS activities.

```bash
aws cloudtrail create-trail \
  --name log-analysis-trail \
  --s3-bucket-name cloudtrail-logs-{account}-{region} \
  --include-global-service-events \
  --is-multi-region-trail
```

## Advanced Features and Optimizations

### Cost Optimization Strategies

Managing storage costs becomes crucial as log volumes grow over time. Implementing intelligent data lifecycle management policies ensures that frequently accessed data remains readily available while older logs transition to more cost-effective storage classes automatically.

```typescript
// Add to your S3 bucket configuration
const logsBucket = new s3.Bucket(this, 'CloudTrailLogsBucket', {
  // ... existing configuration
  lifecycleRules: [{
    id: 'log-lifecycle',
    enabled: true,
    transitions: [{
      storageClass: s3.StorageClass.INFREQUENT_ACCESS,
      transitionAfter: cdk.Duration.days(30)
    }, {
      storageClass: s3.StorageClass.GLACIER,
      transitionAfter: cdk.Duration.days(90)
    }, {
      storageClass: s3.StorageClass.DEEP_ARCHIVE,
      transitionAfter: cdk.Duration.days(365)
    }],
    expiration: cdk.Duration.days(2557) // 7 years
  }]
});
```

### Enhanced Security Analysis with Machine Learning

Advancing beyond rule-based detection, machine learning capabilities can identify subtle behavioral anomalies that traditional approaches might miss. Amazon Personalize can learn normal user behavior patterns and flag deviations that could indicate compromised accounts or insider threats.

```typescript
import { PersonalizeClient, GetRecommendationsCommand } from '@aws-sdk/client-personalize';

export class MLAnomalyDetector {
  private personalizeClient: PersonalizeClient;

  constructor() {
    this.personalizeClient = new PersonalizeClient({ region: process.env.AWS_REGION });
  }

  async detectBehaviorAnomalies(userId: string, currentActivity: any[]): Promise<number> {
    // Use Amazon Personalize to detect unusual user behavior patterns
    const command = new GetRecommendationsCommand({
      campaignArn: process.env.PERSONALIZE_CAMPAIGN_ARN,
      userId: userId,
      numResults: 10
    });

    const response = await this.personalizeClient.send(command);
    
    // Compare current activity against expected behavior patterns
    // Return anomaly score (0-10)
    return this.calculateAnomalyScore(currentActivity, response.itemList);
  }

  private calculateAnomalyScore(current: any[], expected: any[]): number {
    // Implement ML-based scoring logic
    // This is a simplified example
    return Math.random() * 10; // Replace with actual ML logic
  }
}
```

### Real-time Processing Capabilities

While batch processing handles the bulk of log analysis efficiently, certain high-risk events require immediate attention. Adding Kinesis Data Streams enables real-time event processing for critical security events that demand instant response.

```typescript
import { KinesisClient, PutRecordCommand } from '@aws-sdk/client-kinesis';

export class RealTimeProcessor {
  private kinesisClient: KinesisClient;

  constructor() {
    this.kinesisClient = new KinesisClient({ region: process.env.AWS_REGION });
  }

  async processRealTimeEvent(event: any): Promise<void> {
    const riskScore = await this.calculateRiskScore(event);
    
    if (riskScore >= 8) {
      // High-risk event - immediate processing
      await this.triggerImmediateAnalysis(event);
    }

    // Stream to Kinesis for batch processing
    const command = new PutRecordCommand({
      StreamName: process.env.KINESIS_STREAM_NAME,
      Data: Buffer.from(JSON.stringify(event)),
      PartitionKey: event.userId || event.sourceIP
    });

    await this.kinesisClient.send(command);
  }

  private async triggerImmediateAnalysis(event: any): Promise<void> {
    // Trigger immediate security response
    // Could involve Lambda, SNS, or Step Functions
  }

  private async calculateRiskScore(event: any): Promise<number> {
    // Implement real-time risk scoring
    return 0;
  }
}
```

## Monitoring and Observability

Implement comprehensive monitoring for your log analysis system:

```typescript
import { CloudWatchClient, PutMetricDataCommand } from '@aws-sdk/client-cloudwatch';

export class MetricsCollector {
  private cloudWatchClient: CloudWatchClient;

  constructor() {
    this.cloudWatchClient = new CloudWatchClient({ region: process.env.AWS_REGION });
  }

  async recordProcessingMetrics(eventsProcessed: number, processingTime: number, errors: number): Promise<void> {
    const command = new PutMetricDataCommand({
      Namespace: 'LogAnalysis',
      MetricData: [{
        MetricName: 'EventsProcessed',
        Value: eventsProcessed,
        Unit: 'Count'
      }, {
        MetricName: 'ProcessingLatency',
        Value: processingTime,
        Unit: 'Milliseconds'
      }, {
        MetricName: 'ProcessingErrors',
        Value: errors,
        Unit: 'Count'
      }]
    });

    await this.cloudWatchClient.send(command);
  }

  async recordAnomalyMetrics(anomalies: AnomalyReport[]): Promise<void> {
    const criticalCount = anomalies.filter(a => a.severity === 'CRITICAL').length;
    const highCount = anomalies.filter(a => a.severity === 'HIGH').length;

    const command = new PutMetricDataCommand({
      Namespace: 'SecurityAnalysis',
      MetricData: [{
        MetricName: 'CriticalAnomalies',
        Value: criticalCount,
        Unit: 'Count'
      }, {
        MetricName: 'HighRiskAnomalies',
        Value: highCount,
        Unit: 'Count'
      }]
    });

    await this.cloudWatchClient.send(command);
  }
}
```

## Security Best Practices

### IAM Policy Implementation

Security in a log analysis system begins with properly configured IAM policies that follow the principle of least privilege. Each Lambda function should have access only to the specific resources it needs to perform its designated tasks, reducing the potential impact of any security compromise.

```typescript
// Add to your CDK stack
const processorPolicy = new iam.PolicyDocument({
  statements: [
    new iam.PolicyStatement({
      effect: iam.Effect.ALLOW,
      actions: [
        's3:GetObject'
      ],
      resources: [`${logsBucket.bucketArn}/*`]
    }),
    new iam.PolicyStatement({
      effect: iam.Effect.ALLOW,
      actions: [
        'dynamodb:PutItem',
        'dynamodb:GetItem'
      ],
      resources: [analysisTable.tableArn]
    }),
    new iam.PolicyStatement({
      effect: iam.Effect.ALLOW,
      actions: [
        'sns:Publish'
      ],
      resources: [alertTopic.topicArn]
    })
  ]
});
```

### Data Encryption Requirements

Protecting sensitive log data requires encryption both at rest and in transit. AWS managed encryption keys provide a balance between security and operational simplicity, while ensuring that all stored data meets compliance requirements for data protection.

```typescript
// Add encryption to your resources
const analysisTable = new dynamodb.Table(this, 'AnalysisTable', {
  // ... existing configuration
  encryption: dynamodb.TableEncryption.AWS_MANAGED,
  pointInTimeRecovery: true
});

const logsBucket = new s3.Bucket(this, 'CloudTrailLogsBucket', {
  // ... existing configuration
  encryption: s3.BucketEncryption.S3_MANAGED,
  enforceSSL: true
});
```

## Conclusion

This serverless log analysis system provides a scalable, cost-effective solution for monitoring AWS environments. By combining CloudTrail data processing, AI-powered analysis, and automated alerting, you can gain deep insights into your infrastructure activity while minimizing operational overhead.

The architecture delivers automatic scaling based on log volume, ensuring that your monitoring capabilities grow seamlessly with your infrastructure demands. Cost optimization occurs naturally through serverless pricing models, where you only pay for the compute resources actually consumed during log processing and analysis. The intelligent analysis capabilities use AI to identify patterns that humans might miss, providing deeper insights into potential security threats and operational issues that traditional rule-based systems often overlook.

Real-time alerting ensures that critical security events receive immediate attention, while comprehensive reporting capabilities support compliance requirements and governance initiatives. The system can be extended with additional data sources, more sophisticated machine learning models, or integration with external security tools to meet evolving organizational needs.

As your AWS environment grows, this serverless architecture will scale seamlessly to handle increasing log volumes while maintaining consistent performance and reliability. The modular design allows for incremental improvements and feature additions without disrupting existing functionality.

Remember to regularly review and tune your anomaly detection rules, update your AI prompts based on new threat patterns, and continuously monitor the system's performance to ensure optimal security posture for your AWS infrastructure.
